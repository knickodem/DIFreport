---
title: "WB Project Decisions"
output:
  html_document:
    toc: yes
always_allow_html: yes
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Identifying data errors

**EGMA_number_recognition.Endline**: The stage1 MH test produced a statistic, df, and p-value, but no estimate or CI.

```{r data error check}
library(dplyr)
check <- MalawiData[384:403] %>%
  tidyr::gather(Item, Response) %>%
  group_by(Item, Response) %>% summarize(n = n()) %>%
  mutate(r = n())
```

 - recog4_3 has a value of 3 in one cell (instead of all 0s and 1s)
 - recog12_3 has a value of 2 in one cell
 - recog15_3 has a value of 9 in one cell
 

# Alternative approaches

Comparing results from our code to using the difR package as a sensitivity check
 - Would need to write a function to extract the relevant info into the dataframe output we desire since the difR output is disjointed
 - Also a bit difficult to determine what is going on "under the hood" in difR. Need to review the workhorse functions to determine what procedures are exactly being used, and how many tests are conducted, etc...
 - At first glance, it does not seem like using difR would save much time/debugging issues 



# Decisions, assumptions, and defaults

I am currently organizing this by function, which should be helpful if this transformed to an r package

## WB_Data_Prep

 - Assumes the last 2 characters in the item (column) names can be removed
 - Only removes cases with missing data on all items or in the grouping variable
 - Recodes NAs to 0
   - Thus, scores could be a product of motivation rather than ability; in other words, the analysis equates low motivation to low ability  



## DIF_analysis

 - Assumes the grouping variable (groupvec) is a factor with 2 levels


